\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}

\title{Real-Time Transit Data Pipeline\\A Streaming Architecture for Swedish Public Transport}
\author{Pranav Rajan \and David Tanudin}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}

This project implements a real-time data processing pipeline for Swedish public transport data, developed as part of the ID2221 Data-Intensive Computing course at KTH. The system demonstrates practical applications of streaming architectures, NoSQL databases, and distributed processing frameworks. We ingest live transit data from Trafiklab's API and make it accessible through an interactive web dashboard, enabling users to monitor real-time departures and delay patterns across Sweden's public transport network.

\section{Dataset}

\subsection{Data Source}
We utilize Trafiklab's real-time departures API (\url{https://www.trafiklab.se/}), which provides comprehensive transit data for Swedish public transport. The API offers two key endpoints: (1) a stops search API that returns transit stop metadata including stop IDs, names, and transport modes, and (2) a departures API that returns scheduled and real-time departure information.

\subsection{Data Structure}
Each departure record contains the following fields:
\begin{itemize}
    \item \textbf{Operator}: Transit agency name (e.g., SL, Västtrafik)
    \item \textbf{Line}: Route designation (e.g., "14", "Blue Line")
    \item \textbf{Destination}: Terminal station or direction
    \item \textbf{Scheduled Time}: Planned departure timestamp
    \item \textbf{Real-time Time}: Actual departure based on vehicle tracking
    \item \textbf{Delay}: Difference in seconds between scheduled and real-time
    \item \textbf{Stop ID}: Unique identifier for the transit stop
\end{itemize}

The API returns all departures within a 60-minute window from the query time, with a monthly quota of 100,000 calls, which is sufficient for continuous monitoring of multiple stops.

\section{Methodology}

\subsection{Architecture}
Our pipeline follows a standard streaming architecture: \texttt{Trafiklab API → Producer → Kafka → Spark Streaming → MongoDB → Web Dashboard}. Each component serves a distinct purpose:

\textbf{Producer}: A Python script polls the Trafiklab API every 60 seconds and publishes JSON messages to Kafka. The producer accepts a stop ID as a command-line argument, allowing monitoring of any transit stop in Sweden.

\textbf{Kafka}: Acts as the message broker, decoupling data ingestion from processing. Messages are published to the \texttt{sl\_stream} topic with configurable retention and replication.

\textbf{Spark Streaming}: Consumes messages from Kafka in micro-batches, applies schema validation, and writes to MongoDB. Spark ensures fault tolerance through checkpointing and enables scalable processing.

\textbf{MongoDB}: Stores all departure records in a document-oriented format. The flexible schema accommodates varying data structures from different operators.

\textbf{Web Dashboard}: A Flask application with automatic producer management. Users can search for stops, select a stop to monitor, and view real-time departures with pagination and auto-refresh.

\subsection{Mini-Batch Processing}
We employ a 60-second mini-batch interval, which aligns with the API's data characteristics. Since the API returns departures within a 60-minute window, sub-minute polling would yield redundant data. However, delay information updates continuously based on vehicle GPS positions, making the 60-second refresh optimal for capturing real-time changes without excessive API calls.

\subsection{Deduplication Strategy}
The API's 60-minute window creates duplicate records across consecutive fetches. We implement MongoDB aggregation pipelines that group departures by composite key (operator, line, destination, scheduled time) and retain only the most recent record. This ensures users see unique departures with the latest delay estimates.

\subsection{Producer Management}
The web dashboard features automatic producer management. When a user selects a transit stop, the system spawns a dedicated producer subprocess for that stop and terminates any previous producers. This optimizes resource usage while maintaining historical data in MongoDB, enabling instant switching between stops.

\section{Results}

\subsection{System Performance}
The pipeline successfully processes real-time transit data with end-to-end latency of approximately 60-75 seconds (60s for API polling + 10-15s for Kafka/Spark/MongoDB processing). The system handles multiple concurrent stops and scales horizontally through Kafka partitioning and Spark executors.

\subsection{Data Quality}
Our deduplication strategy reduces database size by approximately 60\%, eliminating redundant records while preserving the most current delay information. At major stations like T-Centralen in Stockholm, we observe frequent delay updates across 50+ unique departures per hour.

\subsection{User Experience}
The web dashboard provides an intuitive interface for transit monitoring. Users can search Swedish transit stops, view paginated departures (50 per page), and observe live countdown timers. The auto-refresh feature ensures data remains current without manual intervention.

\section{Running the Code}

Detailed instructions for running the code are available in the README.md file in the project repository on GitHub. A complete copy of these instructions is also provided in Appendix A for reference.

\section{Conclusion}

This project demonstrates a complete streaming data pipeline that balances real-time requirements with practical constraints. The 60-second mini-batch approach captures meaningful delay updates without excessive API overhead. The combination of Kafka for message delivery, Spark for distributed processing, and MongoDB for flexible storage creates a robust foundation for real-time transit monitoring. The system provides valuable insights into public transport performance, particularly at high-traffic stations where delay patterns emerge clearly in the real-time data stream.

\appendix

\section{Installation and Setup Instructions}

\subsection{Prerequisites}
\begin{itemize}
    \item Python 3.11+
    \item Docker Desktop with Docker Compose
    \item Trafiklab API key is included in the repository's \texttt{.env} file
\end{itemize}

\subsection{Installation Steps}
\textbf{1. Clone repository and install dependencies:}
\begin{verbatim}
git clone <repository-url>
cd id2221-data-intensive-traffic
pip install -r requirements.txt
\end{verbatim}

\textbf{2. Verify API key:}\\
The \texttt{.env} file with the Trafiklab API key is already included in the GitHub repository. Verify it exists after cloning:
\begin{verbatim}
cat .env
\end{verbatim}

\textbf{3. Start the pipeline:}
\begin{verbatim}
python main.py
\end{verbatim}
This command starts Docker containers (Kafka, Spark, MongoDB), initializes the producer, and begins Spark streaming.

\textbf{4. Start the web dashboard (separate terminal):}
\begin{verbatim}
python web_dashboard.py
\end{verbatim}

\textbf{5. Access the dashboard:}\\
Open \url{http://localhost:5432} in a web browser. Search for a transit stop (e.g., "Stockholm", "Göteborg"), select it, and wait 60 seconds for the first data to appear.

\subsection{Available Services}
\begin{itemize}
    \item \textbf{Web Dashboard}: \url{http://localhost:5432}
    \item \textbf{Kafka UI (Kafdrop)}: \url{http://localhost:9000}
    \item \textbf{MongoDB}: \texttt{mongodb://localhost:27017}
\end{itemize}

\subsection{Querying MongoDB}
To query data directly:
\begin{verbatim}
python mongo_query.py
\end{verbatim}

Or use MongoDB shell:
\begin{verbatim}
docker exec -it mongodb mongosh
use trafiklab
db.departures.find().limit(5).pretty()
\end{verbatim}

\end{document}
