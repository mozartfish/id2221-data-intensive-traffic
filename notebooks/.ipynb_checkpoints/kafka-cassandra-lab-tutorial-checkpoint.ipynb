{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7ffb7c-b05e-4efc-a098-bf1ad9bf11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65c253-1a63-46a2-9e05-298c40d35b6b",
   "metadata": {},
   "source": [
    "## Create Kafka Topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0451e6c3-c6d5-4ca8-b98b-2264116a977f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Kafka topic...\n",
      "Topic 'test' already exists\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Kafka topic...\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    client_id='notebook_admin'\n",
    ")\n",
    "\n",
    "topic_name = 'test'\n",
    "topic = NewTopic(\n",
    "    name=topic_name, \n",
    "    num_partitions=1, \n",
    "    replication_factor=1\n",
    ")\n",
    "\n",
    "try:\n",
    "    admin_client.create_topics(new_topics=[topic], validate_only=False)\n",
    "    print(f\"Topic '{topic_name}' created\")\n",
    "except TopicAlreadyExistsError:\n",
    "    print(f\"Topic '{topic_name}' already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    admin_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739e9f9-bca1-492f-b36a-820b5f5c3f9b",
   "metadata": {},
   "source": [
    "## Create Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d81cd2-2efd-4ae6-ba5d-7055261947f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent 5 messages\n"
     ]
    }
   ],
   "source": [
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: v.encode('utf-8')\n",
    ")\n",
    "rows = [\n",
    "    \"196,242,3.0,881250949\",\n",
    "    \"186,302,3.0,891717742\",\n",
    "    \"22,377,1.0,878887116\",\n",
    "    \"244,51,2.0,880606923\",\n",
    "    \"166,346,1.0,886397596\",\n",
    "]\n",
    "\n",
    "for line in rows:\n",
    "    producer.send('test', value=line)\n",
    "    \n",
    "producer.flush()\n",
    "producer.close()\n",
    "print(f\"Sent {len(rows)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c2000-b923-4e43-bb0c-a063010999ac",
   "metadata": {},
   "source": [
    "## Cassandra and Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6f38cc-0efa-4fe3-9b77-1e639349d496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Spark with Cassandra connector...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/Users/pranavrajan/Desktop/id2221-data-intensive-traffic/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/pranavrajan/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/pranavrajan/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      "org.apache.spark#spark-token-provider-kafka-0-10_2.13 added as a dependency\n",
      "com.datastax.spark#spark-cassandra-connector_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bfacfefd-bb62-463d-80f4-66e74d37d34b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.0 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.13;3.4.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.13;3.4.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.13;2.11.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.13.11 in central\n",
      ":: resolution report :: resolve 383ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.13;3.4.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.13;3.4.1 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.13.11 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.13;2.11.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 by [org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;2.0.16] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   31  |   0   |   0   |   3   ||   28  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bfacfefd-bb62-463d-80f4-66e74d37d34b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 28 already retrieved (0kB/7ms)\n",
      "25/09/29 14:15:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://n191-p113.eduroam.kth.se:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>kafka-cassandra-streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10ba75ed0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nInitializing Spark with Cassandra connector...\")\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"kafka-cassandra-streaming\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0,\"\n",
    "        \"org.apache.spark:spark-token-provider-kafka-0-10_2.13:4.0.0,\"\n",
    "        \"com.datastax.spark:spark-cassandra-connector_2.13:3.4.1\"\n",
    "    )\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\")\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark session created!\")\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5fbbf7e-8e95-4bc2-9e8e-bf7d5a37b67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading stream from Kafka...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReading stream from Kafka...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c5855-8b0d-4949-b830-b750c2a62781",
   "metadata": {},
   "source": [
    "## Create Schema for Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32cf946b-c0a0-49a6-9c71-0bebdd8d3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, LongType\n",
    "from pyspark.sql.functions import col, from_csv\n",
    "\n",
    "# Schema for the CSV in Kafka 'value'\n",
    "schema_ddl = \"UserId INT, MovieId INT, Rating DOUBLE, Timestamp LONG\"\n",
    "\n",
    "# Read from Kafka\n",
    "raw = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"test\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a39e357d-80a3-468f-9a2d-57d2b4b6a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing Kafka messages...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nParsing Kafka messages...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b9a1d1-bc7f-40dd-a08c-e5e3c4e9032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = (raw\n",
    "    .selectExpr(\"CAST(value AS STRING) AS value_str\", \"timestamp AS kafka_timestamp\")\n",
    "    .select(\n",
    "        from_csv(col(\"value_str\"), schema_ddl).alias(\"r\"), \n",
    "        col(\"kafka_timestamp\")\n",
    "    )\n",
    "    .select(\"r.*\", \"kafka_timestamp\")\n",
    "    .where(col(\"UserId\").isNotNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d667574-1162-4150-86df-2ddb7ad91e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write stream to Cassandra!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nWrite stream to Cassandra!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39992001-1469-4252-b128-ae45448f3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_cassandra(batch_df, epoch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        print(f\"Batch {epoch_id}: No data\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Batch {epoch_id} ---\")\n",
    "    batch_df.show()\n",
    "\n",
    "    batch_df.write \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"keyspace\", \"movie_ratings\") \\\n",
    "        .option(\"table\", \"ratings\") \\\n",
    "        .save()\n",
    "\n",
    "    count = batch_df.count()\n",
    "    print(f\"Wrote {count} rows to cassandra!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "542836b7-302c-4447-8220-b6d0b0323368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming query!!!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStreaming query!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "754b3c87-c540-45da-b532-e634c6a78519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 14:15:35 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/73/y8z3_15n0c3fnwk5fkhj6ql40000gn/T/temporary-1bd9a033-231d-4146-ab78-9f0355d332c1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/09/29 14:15:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch 0 ---\n",
      "+------+-------+------+---------+--------------------+\n",
      "|UserId|MovieId|Rating|Timestamp|     kafka_timestamp|\n",
      "+------+-------+------+---------+--------------------+\n",
      "|   196|    242|   3.0|881250949|2025-09-29 13:12:...|\n",
      "|   186|    302|   3.0|891717742|2025-09-29 13:12:...|\n",
      "|    22|    377|   1.0|878887116|2025-09-29 13:12:...|\n",
      "|   244|     51|   2.0|880606923|2025-09-29 13:12:...|\n",
      "|   166|    346|   1.0|886397596|2025-09-29 13:12:...|\n",
      "|   196|    242|   3.0|881250949|2025-09-29 14:14:...|\n",
      "|   186|    302|   3.0|891717742|2025-09-29 14:14:...|\n",
      "|    22|    377|   1.0|878887116|2025-09-29 14:14:...|\n",
      "|   244|     51|   2.0|880606923|2025-09-29 14:14:...|\n",
      "|   166|    346|   1.0|886397596|2025-09-29 14:14:...|\n",
      "|   196|    242|   3.0|881250949|2025-09-29 14:15:...|\n",
      "|   186|    302|   3.0|891717742|2025-09-29 14:15:...|\n",
      "|    22|    377|   1.0|878887116|2025-09-29 14:15:...|\n",
      "|   244|     51|   2.0|880606923|2025-09-29 14:15:...|\n",
      "|   166|    346|   1.0|886397596|2025-09-29 14:15:...|\n",
      "+------+-------+------+---------+--------------------+\n",
      "\n",
      "Wrote 15 rows to cassandra!\n"
     ]
    }
   ],
   "source": [
    "query = (parsed.writeStream\n",
    "    .foreachBatch(write_to_cassandra)\n",
    "    .outputMode(\"append\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ebd8a09-87eb-4ec2-8dd1-52c647df9ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream for 15 seconds\n",
      "\n",
      "Stream completed successfully\n",
      "\n",
      "Stream stopped!\n"
     ]
    }
   ],
   "source": [
    "# In[13]:\n",
    "print(\"\\nStream for 15 seconds\")\n",
    "\n",
    "try:\n",
    "    query.awaitTermination(15)\n",
    "    print(\"\\nStream completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nStream failed: {e}\")\n",
    "finally:\n",
    "    if query.isActive:\n",
    "        query.stop()\n",
    "    print(\"\\nStream stopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ce817d0-a9d1-4ac6-a168-9135dbe46790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Cassandra!!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nQuery Cassandra!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8729d3df-4c3c-4efa-97d0-569d3926f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassandra_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"movie_ratings\") \\\n",
    "    .option(\"table\", \"ratings\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d6dac4-0e62-4e80-bf96-afa987dd3c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 records\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = cassandra_df.count()\n",
    "print(f\"{total_count} records\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a9c297e-7cff-4ee5-bb01-fb8d95950a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+------+-----------------------+\n",
      "|UserId|MovieId|Timestamp|Rating|kafka_timestamp        |\n",
      "+------+-------+---------+------+-----------------------+\n",
      "|244   |51     |880606923|2.0   |2025-09-29 14:15:31.165|\n",
      "|22    |377    |878887116|1.0   |2025-09-29 14:15:31.165|\n",
      "|166   |346    |886397596|1.0   |2025-09-29 14:15:31.165|\n",
      "|186   |302    |891717742|3.0   |2025-09-29 14:15:31.165|\n",
      "|196   |242    |881250949|3.0   |2025-09-29 14:15:31.165|\n",
      "+------+-------+---------+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cassandra_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a428e47-d7cd-4fa6-9fd3-3ece0d84d4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserId: integer (nullable = false)\n",
      " |-- MovieId: integer (nullable = true)\n",
      " |-- Timestamp: long (nullable = true)\n",
      " |-- Rating: double (nullable = true)\n",
      " |-- kafka_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cassandra_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90b62afa-1ac0-429b-9d6d-7bc5b9566c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+------+-----------------------+\n",
      "|UserId|MovieId|Timestamp|Rating|kafka_timestamp        |\n",
      "+------+-------+---------+------+-----------------------+\n",
      "|166   |346    |886397596|1.0   |2025-09-29 14:15:31.165|\n",
      "|186   |302    |891717742|3.0   |2025-09-29 14:15:31.165|\n",
      "|244   |51     |880606923|2.0   |2025-09-29 14:15:31.165|\n",
      "|22    |377    |878887116|1.0   |2025-09-29 14:15:31.165|\n",
      "|196   |242    |881250949|3.0   |2025-09-29 14:15:31.165|\n",
      "+------+-------+---------+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from Cassandra\n",
    "cassandra_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"movie_ratings\") \\\n",
    "    .option(\"table\", \"ratings\") \\\n",
    "    .load()\n",
    "\n",
    "cassandra_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
